 \chapter{Introduction}

This thesis is organized into two parts.
The first part describes a statistical combination of three most sensitive dark matter searches and a summary of other analyses interpreted in the Two-Higgs-Doublet
Model extended by a pseudoscalar mediator (\thdma) using 139 \ifb of 
proton-proton collision data at center-of-mass energy $\sqrt{s}=13$ \TeV collected by the ATLAS detector during Run 3 of the 
Large Hadron Collider. 
The second reports the development of an algorithm for charged-particle track reconstruction using Graph Neural Networks (GNNs) for the ATLAS 
Inner Tracker (ITk), commissioned for the High Luminosity phase of the Large Hadron Collider (HL-LHC).
A brief introduction and motivation of each part is provided in this chapter.

The existence of Dark Matter (DM) enjoys well-established acceptance among particle physicists and cosmologists, supported by a 
wealth of evidence from astrophysical observations~\cite{Corbelli:1999af,Rubin:1980,Begemann:1991,Hinshaw:2012aka,Akrami:2018vks,Trimble1987,Bertone2005,Feng2010}. 
Although DM accounts for an estimated 85\% of the mass in the universe, no known particle candidate in the Standard Model can explain it.
Thus, an explanation of its nature is a central focus of the physics programme in the collider experiments such as ATLAS and CMS~\cite{adan2023darkmattersearchescms}. 
Among the proposed hypotheses, Weakly-Interacting Massive Particles \cite{Steigman:1984ac} (WIMPs) $\chi$ gain much interest for experimental detection of DM, which typically targets the associated production of a visible mediator decaying to stable particles and a large missing transverse momentum with magnitude \met.
The result of these indirect searches is usually interpreted in the context of a simplified model that involves a fermionic DM particle connected to the visible sector via a vector, axial-vector, scalar, or pseudo-scalar mediator $a$. 
It contains a minimal set of free parameters, including the masses and coupling strength of the DM and mediator particles. 

The Two-Higgs-Doublet Model \cite{Bauer:2017ota} (2HDM) plus a pseudo-scalar mediator $a$ is the simplest gauge-invariant and renormalizable extension of the simplified pseudo-scalar DM model, offering a rich phenomenology and a more complete benchmark.
It is identified by the LHC Dark Matter Working Group along with a set of recommended scans to explore its parameter space through LHC searches~\cite{2HDMWGproxi}. 
A variety of analyses using 137 \ifb of proton-proton collision data collected at the center-of-mass energy up to 13 \TeV by the ATLAS detector and targetting diverse visible signatures provide constraints on the \thdma. 
The first part of this thesis presents a statistical combination of the three most sensitive analyses and a summary of the remaining searches.
The statistical combination considers $\met+X$-type signatures where X is either a SM Higgs boson decaying into a pair of $b$-quarks~\cite{EXOT-2018-46} or a $Z$-boson decaying into a pair of leptons~\cite{HIGG-2018-26}, and a search for associated production of a top and a bottom quark with a charged Higgs boson decaying into a top and a bottom quark~\cite{HDBS-2018-51}.
The result from searches targeting signatures are summarized in the result. 
% Overall, this work represents the most comprehensive set of constraints on the model obtained by the ATLAS Collaboration to date. 

These searches for DM leave a sizeable part of the parameter space unexcluded and await more data to derive better constraints on the \thdma. 
In general, searches for exotic phenomena at the LHC are often statistically limited, motivating a substantial boost in the rate of data collection. 
The HL-HLC will help satisfy the demand for data with up to 3-fold increase in instantaneous luminosity $\mathcal{L}$, reaching $7.5\times 10^{34}$ $\mathrm{cm}^{-2}\mathrm{s^{-1}}$~\cite{Aberle:2749422}.
The integrated luminosity delivered to each of the general-purpose detectors will total $4000\ifb$ at the end of Run 5, $\sim 10$ times the amount of data collected during the nominal LHC runs. 
This upgrade will bring unprecedented opportunities for physics discovery and precision measurements, and at the same time many challenges to all aspects of data processing, from simulation to reconstruction. 

In particular, the elevated luminosity leads to an increased number of proton-proton interactions in a bunch crossing.
As a result, events characterized by a large momentum transfer and often producing interesting physics, or hard scatters, are more likely to occur, they are accompanied by a larger background of soft inelastic collisions, or pile-up. 
The expected pile-up $\expval{\mu}$ will increase from $\sim50$ in Run 3 to $140$ at the end of Run 4, peaking at 200 in Run 5, and with it a steep increase in event complexity as well as the necessary CPU resources to for event reconstruction.
The computing budget dedicated to reconstruction is typically dominated by inner tracking, which is why both many LHC experiments are investigating methods based on modern hardware accelerators (GPUs, FPGAs) as a potential solution to this problem. 
In this direction, a tracking algorithm centered on GNNs is identified as a promising approach whose development and evaluation are reported in the second part of this thesis. 


